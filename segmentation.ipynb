{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "IMAGE_DIR = \"input_images\"\n",
    "OUTPUT_DIR = \"segmented_output\"\n",
    "MODEL_NAME = \"nvidia/segformer-b0-finetuned-cityscapes-768-768\"\n",
    "\n",
    "# Cityscapes class mapping\n",
    "CITYSCAPES_ID2LABEL = {\n",
    "    0: 'road', 1: 'sidewalk', 2: 'building', 3: 'wall', 4: 'fence',\n",
    "    5: 'pole', 6: 'traffic_light', 7: 'traffic_sign', 8: 'vegetation', 9: 'terrain',\n",
    "    10: 'sky', 11: 'person', 12: 'rider', 13: 'car', 14: 'truck',\n",
    "    15: 'bus', 16: 'train', 17: 'motorcycle', 18: 'bicycle',\n",
    "}\n",
    "\n",
    "# === SETUP ===\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "for class_name in CITYSCAPES_ID2LABEL.values():\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, class_name), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(MODEL_NAME).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_save(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # (1, num_classes, h/4, w/4)\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=image.size[::-1],  # (H, W)\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        predicted = upsampled_logits.argmax(dim=1)[0].cpu().numpy()  # (H, W)\n",
    "\n",
    "    for class_idx, class_name in CITYSCAPES_ID2LABEL.items():\n",
    "        mask = (predicted == class_idx).astype(np.uint8) * 255\n",
    "        if np.any(mask):\n",
    "            mask_img = Image.fromarray(mask)\n",
    "            base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            mask_img.save(os.path.join(OUTPUT_DIR, class_name, f\"{base_name}_{class_name}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "for filename in tqdm(image_files, desc=\"Segmenting images\"):\n",
    "    segment_and_save(os.path.join(IMAGE_DIR, filename))\n",
    "\n",
    "print(\"Done. Masks saved in:\", OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
