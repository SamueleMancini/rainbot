{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c14548",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "* TRAINING\n",
    "\n",
    "\n",
    "* RECOMMENDED: (training speed-up) Precompute and cache each submodel’s softmax outputs once, store them on disk, and then train the meta‐learner on those saved vectors. This removes the four forward passes during each epoch.\n",
    "\n",
    "\n",
    "* OPTIONAL: chatgpt recommends to not use the same train set for training ensemble model --> check this\n",
    "\n",
    "\n",
    "* OPTIONAL: look at the difference between using one-hidden layer or not (BIG DIFFERENCE)\n",
    "\n",
    "\n",
    "* OPTIONAL: improve trainig loop (with optional interrupt key and saving model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc7a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    top_k_accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40c2f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute absolute path to the `src/` folder\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "SRC_PATH     = os.path.join(PROJECT_ROOT, \"src\")\n",
    "\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "\n",
    "from utils import get_dataloaders, load_model, evaluate_model, print_metrics, plot_confusion_matrix, show_sample_predictions, plot_random_image_with_label_and_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "354266dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9facaa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a9101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES = [\"Albania\",\"Andorra\",\"Argentina\",\"Australia\",\"Austria\",\"Bangladesh\",\"Belgium\",\"Bhutan\",\"Bolivia\",\"Botswana\",\"Brazil\",\"Bulgaria\",\"Cambodia\",\"Canada\",\"Chile\",\"Colombia\",\"Croatia\",\"Czechia\",\"Denmark\",\"Dominican Republic\",\"Ecuador\",\"Estonia\",\"Eswatini\",\"Finland\",\"France\",\"Germany\",\"Ghana\",\"Greece\",\"Greenland\",\"Guatemala\",\"Hungary\",\"Iceland\",\"Indonesia\",\"Ireland\",\"Israel\",\"Italy\",\"Japan\",\"Jordan\",\"Kenya\",\"Kyrgyzstan\",\"Latvia\",\"Lesotho\",\"Lithuania\",\"Luxembourg\",\"Malaysia\",\"Mexico\",\"Mongolia\",\"Montenegro\",\"Netherlands\",\"New Zealand\",\"Nigeria\",\"North Macedonia\",\"Norway\",\"Palestine\",\"Peru\",\"Philippines\",\"Poland\",\"Portugal\",\"Romania\",\"Russia\",\"Senegal\",\"Serbia\",\"Singapore\",\"Slovakia\",\"Slovenia\",\"South Africa\",\"South Korea\",\"Spain\",\"Sri Lanka\",\"Sweden\",\"Switzerland\",\"Taiwan\",\"Thailand\",\"Turkey\",\"Ukraine\",\"United Arab Emirates\",\"United Kingdom\",\"United States\",\"Uruguay\"]\n",
    "num_classes = len(COUNTRIES)\n",
    "project_root   = Path().resolve().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa8101",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6f1be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = project_root/ \"data\" / \"final_datasets\" / \"train\"\n",
    "train_loader = get_dataloaders(train_root, batch_size=32)\n",
    "\n",
    "val_root = project_root/ \"data\" / \"final_datasets\" / \"val\"\n",
    "val_loader = get_dataloaders(val_root, batch_size=32)\n",
    "\n",
    "test_root = project_root/ \"data\" / \"final_datasets\" / \"test\"\n",
    "test_loader = get_dataloaders(test_root, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd170b",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22dbaed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelangelonardi/Desktop/Università/Master/Bocconi Master/Year 1/Semester2/Computer Vision & Image processing/Final - project/rainbot/src/utils.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "base_model = load_model(model_path=project_root / \"models\" / \"resnet_finetuned\" / \"main.pth\", device=device)\n",
    "road = load_model(model_path=project_root / \"models\" / \"resnet_finetuned_road\" / \"main.pth\", device=device)\n",
    "terrain = load_model(model_path=project_root / \"models\" / \"resnet_finetuned_terrain\" / \"main.pth\", device=device)\n",
    "vegetation = load_model(model_path=project_root / \"models\" / \"resnet_finetuned_vegetation\" / \"main.pth\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "009f8ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:103: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"nvidia/segformer-b0-finetuned-cityscapes-768-768\"\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "seg_model = SegformerForSemanticSegmentation.from_pretrained(MODEL_NAME).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbe71900",
   "metadata": {},
   "outputs": [],
   "source": [
    "CITYSCAPES_LABELS = {\n",
    "    0: 'road', \n",
    "    8: 'vegetation',  9: 'terrain'\n",
    "}\n",
    "\n",
    "TARGET_CLASSES = {'road','terrain','vegetation'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321b482",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31008c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to fetch softmax probs from a pretrained submodel\n",
    "def get_probs(model, img_tensor, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(img_tensor.to(device))\n",
    "        probs = nn.functional.softmax(out, dim=1).cpu().squeeze(0).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "096cffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Dataset: wraps CountryImageDataset, runs submodels to produce features\n",
    "class EnsembleDataset(Dataset):\n",
    "    def __init__(self, base_ds, submodels, device):\n",
    "        \"\"\"\n",
    "        base_ds: CountryImageDataset\n",
    "        submodels: dict {'base': base_model, 'road':road_model, ...}\n",
    "        Each model returns a softmax vector of length num_classes.\n",
    "        \"\"\"\n",
    "        self.base_ds   = base_ds\n",
    "        self.submodels = submodels\n",
    "        self.device    = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.base_ds[idx]\n",
    "        x = img.unsqueeze(0)  # add batch dim\n",
    "        # 1) Collect each model's probs\n",
    "        feats = []\n",
    "        for name, m in self.submodels.items():\n",
    "            p = get_probs(m, x, self.device)  # shape (num_classes,)\n",
    "            feats.append(p)\n",
    "        # 2) Concatenate into one feature vector\n",
    "        feature_vector = np.concatenate(feats).astype(np.float32)\n",
    "        return torch.from_numpy(feature_vector), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7147eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Network: one hidden layer\n",
    "class EnsembleNet(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60c1da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, loss_fn, optimizer, device, epoch, log_every=5):\n",
    "    \"\"\"\n",
    "    Runs one epoch of training, printing updates every `log_every` batches.\n",
    "\n",
    "    Args:\n",
    "        model       (nn.Module):      the network to train\n",
    "        loader      (DataLoader):     training data loader\n",
    "        loss_fn     (callable):       loss function\n",
    "        optimizer   (torch.optim.Optimizer)\n",
    "        device      (torch.device)\n",
    "        epoch       (int):            current epoch number (for prints)\n",
    "        log_every   (int):            how many batches between prints\n",
    "\n",
    "    Returns:\n",
    "        avg_loss (float), avg_acc (float)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (imgs, labels) in enumerate(loader, start=1):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        batch_correct = (preds == labels).sum().item()\n",
    "        batch_size = imgs.size(0)\n",
    "\n",
    "        running_loss    += loss.item() * batch_size\n",
    "        running_correct += batch_correct\n",
    "        total_samples   += batch_size\n",
    "\n",
    "        # Log every N batches\n",
    "        if batch_idx % log_every == 0 or batch_idx == len(loader):\n",
    "            batch_loss = running_loss / total_samples\n",
    "            batch_acc  = running_correct / total_samples\n",
    "            print(f\"Epoch {epoch} [{batch_idx}/{len(loader)}]  \"\n",
    "                  f\"Loss: {batch_loss:.4f}  Acc: {batch_acc:.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / total_samples\n",
    "    avg_acc  = running_correct / total_samples\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63513867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            loss   = loss_fn(logits, y)\n",
    "            preds  = logits.argmax(dim=1)\n",
    "            total_correct += (preds==y).sum().item()\n",
    "            total_loss    += loss.item() * X.size(0)\n",
    "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4303784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Wrap submodels\n",
    "submodels = {\n",
    "    'base':     base_model,\n",
    "    'road':     road,\n",
    "    'terrain':  terrain,\n",
    "    'vegetation': vegetation\n",
    "}\n",
    "\n",
    "# b) Build ensemble datasets\n",
    "train_ds = EnsembleDataset(train_loader.dataset, submodels, device)\n",
    "val_ds   = EnsembleDataset(val_loader.dataset,   submodels, device)\n",
    "test_ds  = EnsembleDataset(test_loader.dataset,  submodels, device)\n",
    "\n",
    "train_el = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_el   = DataLoader(val_ds,   batch_size=32)\n",
    "test_el  = DataLoader(test_ds,  batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7ab90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate meta‐model\n",
    "project_root = Path().resolve().parent\n",
    "\n",
    "hid_dim=128\n",
    "epochs=10\n",
    "lr=1e-3\n",
    "\n",
    "in_dim = num_classes * len(submodels)\n",
    "model = EnsembleNet(in_dim, hid_dim, num_classes).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt     = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "ckpt_path = project_root / \"models\" / \"ensemble\" / \"main.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b4f590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [5/889]  Loss: 4.3709  Acc: 0.0312\n",
      "Epoch 1 [10/889]  Loss: 4.3681  Acc: 0.0187\n",
      "Epoch 1 [15/889]  Loss: 4.3612  Acc: 0.0479\n",
      "Epoch 1 [20/889]  Loss: 4.3545  Acc: 0.0766\n",
      "Epoch 1 [25/889]  Loss: 4.3507  Acc: 0.0875\n",
      "Epoch 1 [30/889]  Loss: 4.3437  Acc: 0.1042\n",
      "Epoch 1 [35/889]  Loss: 4.3378  Acc: 0.1179\n",
      "Epoch 1 [40/889]  Loss: 4.3305  Acc: 0.1375\n",
      "Epoch 1 [45/889]  Loss: 4.3239  Acc: 0.1569\n",
      "Epoch 1 [50/889]  Loss: 4.3155  Acc: 0.1831\n",
      "Epoch 1 [55/889]  Loss: 4.3088  Acc: 0.2028\n",
      "Epoch 1 [60/889]  Loss: 4.2989  Acc: 0.2286\n",
      "Epoch 1 [65/889]  Loss: 4.2883  Acc: 0.2505\n",
      "Epoch 1 [70/889]  Loss: 4.2781  Acc: 0.2665\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_el\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     vl_loss, vl_acc \u001b[38;5;241m=\u001b[39m eval_epoch(model, val_el,   loss_fn, \u001b[38;5;28;01mNone\u001b[39;00m, device)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvl_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvl_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 22\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, loss_fn, optimizer, device, epoch, log_every)\u001b[0m\n\u001b[1;32m     19\u001b[0m running_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (imgs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     23\u001b[0m     imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cv_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[46], line 22\u001b[0m, in \u001b[0;36mEnsembleDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m feats \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodels\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 22\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43mget_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (num_classes,)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     feats\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 2) Concatenate into one feature vector\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m, in \u001b[0;36mget_probs\u001b[0;34m(model, img_tensor, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(img_tensor\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m----> 6\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    tr_loss, tr_acc = train_epoch(model, train_el, loss_fn, opt, device, epoch)\n",
    "    vl_loss, vl_acc = eval_epoch(model, val_el,   loss_fn, None, device)\n",
    "    print(f\"Epoch {epoch}: train {tr_loss:.3f}/{tr_acc:.3f} | val {vl_loss:.3f}/{vl_acc:.3f}\")\n",
    "\n",
    "    # Save best\n",
    "    if vl_loss < best_val_loss:\n",
    "        best_val_loss = vl_loss\n",
    "        torch.save(model.state_dict(), ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469920f",
   "metadata": {},
   "source": [
    "With hidden layers in 2 min \n",
    "\n",
    "Epoch 1 [5/889]  Loss: 4.3709  Acc: 0.0312\n",
    "Epoch 1 [10/889]  Loss: 4.3681  Acc: 0.0187\n",
    "Epoch 1 [15/889]  Loss: 4.3612  Acc: 0.0479\n",
    "Epoch 1 [20/889]  Loss: 4.3545  Acc: 0.0766\n",
    "Epoch 1 [25/889]  Loss: 4.3507  Acc: 0.0875\n",
    "Epoch 1 [30/889]  Loss: 4.3437  Acc: 0.1042\n",
    "Epoch 1 [35/889]  Loss: 4.3378  Acc: 0.1179\n",
    "Epoch 1 [40/889]  Loss: 4.3305  Acc: 0.1375\n",
    "Epoch 1 [45/889]  Loss: 4.3239  Acc: 0.1569\n",
    "Epoch 1 [50/889]  Loss: 4.3155  Acc: 0.1831\n",
    "Epoch 1 [55/889]  Loss: 4.3088  Acc: 0.2028\n",
    "Epoch 1 [60/889]  Loss: 4.2989  Acc: 0.2286\n",
    "Epoch 1 [65/889]  Loss: 4.2883  Acc: 0.2505\n",
    "Epoch 1 [70/889]  Loss: 4.2781  Acc: 0.2665"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8f9f1",
   "metadata": {},
   "source": [
    "No hidden layer in 15 min (started from 0 actually)\n",
    "\n",
    "Epoch 1 [5/889]  Loss: 4.2254  Acc: 0.3000\n",
    "Epoch 1 [10/889]  Loss: 4.2103  Acc: 0.3375\n",
    "Epoch 1 [15/889]  Loss: 4.2114  Acc: 0.3229\n",
    "Epoch 1 [20/889]  Loss: 4.2089  Acc: 0.3328\n",
    "Epoch 1 [25/889]  Loss: 4.2062  Acc: 0.3337\n",
    "Epoch 1 [30/889]  Loss: 4.2012  Acc: 0.3458\n",
    "Epoch 1 [35/889]  Loss: 4.1964  Acc: 0.3696\n",
    "Epoch 1 [40/889]  Loss: 4.1932  Acc: 0.3742\n",
    "Epoch 1 [45/889]  Loss: 4.1904  Acc: 0.3792\n",
    "Epoch 1 [50/889]  Loss: 4.1871  Acc: 0.3812\n",
    "Epoch 1 [55/889]  Loss: 4.1824  Acc: 0.3937\n",
    "Epoch 1 [60/889]  Loss: 4.1785  Acc: 0.4005\n",
    "Epoch 1 [65/889]  Loss: 4.1748  Acc: 0.4101\n",
    "Epoch 1 [70/889]  Loss: 4.1718  Acc: 0.4170\n",
    "Epoch 1 [75/889]  Loss: 4.1685  Acc: 0.4217\n",
    "Epoch 1 [80/889]  Loss: 4.1657  Acc: 0.4242\n",
    "Epoch 1 [85/889]  Loss: 4.1624  Acc: 0.4290\n",
    "Epoch 1 [90/889]  Loss: 4.1595  Acc: 0.4323\n",
    "Epoch 1 [95/889]  Loss: 4.1570  Acc: 0.4352\n",
    "Epoch 1 [100/889]  Loss: 4.1532  Acc: 0.4409\n",
    "Epoch 1 [105/889]  Loss: 4.1505  Acc: 0.4458\n",
    "Epoch 1 [110/889]  Loss: 4.1477  Acc: 0.4472\n",
    "Epoch 1 [115/889]  Loss: 4.1455  Acc: 0.4473\n",
    "Epoch 1 [120/889]  Loss: 4.1422  Acc: 0.4523\n",
    "Epoch 1 [125/889]  Loss: 4.1387  Acc: 0.4585\n",
    "Epoch 1 [130/889]  Loss: 4.1355  Acc: 0.4620\n",
    "Epoch 1 [135/889]  Loss: 4.1324  Acc: 0.4644\n",
    "Epoch 1 [140/889]  Loss: 4.1296  Acc: 0.4656\n",
    "Epoch 1 [145/889]  Loss: 4.1267  Acc: 0.4690\n",
    "Epoch 1 [150/889]  Loss: 4.1238  Acc: 0.4700\n",
    "Epoch 1 [155/889]  Loss: 4.1206  Acc: 0.4736\n",
    "Epoch 1 [160/889]  Loss: 4.1174  Acc: 0.4770\n",
    "Epoch 1 [165/889]  Loss: 4.1144  Acc: 0.4797\n",
    "Epoch 1 [170/889]  Loss: 4.1113  Acc: 0.4825\n",
    "Epoch 1 [175/889]  Loss: 4.1082  Acc: 0.4855\n",
    "Epoch 1 [180/889]  Loss: 4.1053  Acc: 0.4884\n",
    "Epoch 1 [185/889]  Loss: 4.1025  Acc: 0.4900\n",
    "Epoch 1 [190/889]  Loss: 4.0994  Acc: 0.4914\n",
    "Epoch 1 [195/889]  Loss: 4.0966  Acc: 0.4933\n",
    "Epoch 1 [200/889]  Loss: 4.0933  Acc: 0.4980\n",
    "Epoch 1 [205/889]  Loss: 4.0902  Acc: 0.5012\n",
    "Epoch 1 [210/889]  Loss: 4.0867  Acc: 0.5045\n",
    "Epoch 1 [215/889]  Loss: 4.0839  Acc: 0.5061\n",
    "Epoch 1 [220/889]  Loss: 4.0807  Acc: 0.5078\n",
    "Epoch 1 [225/889]  Loss: 4.0776  Acc: 0.5108\n",
    "Epoch 1 [230/889]  Loss: 4.0745  Acc: 0.5130\n",
    "Epoch 1 [235/889]  Loss: 4.0717  Acc: 0.5160\n",
    "Epoch 1 [240/889]  Loss: 4.0684  Acc: 0.5188\n",
    "Epoch 1 [245/889]  Loss: 4.0654  Acc: 0.5213\n",
    "Epoch 1 [250/889]  Loss: 4.0622  Acc: 0.5234\n",
    "Epoch 1 [255/889]  Loss: 4.0599  Acc: 0.5241\n",
    "Epoch 1 [260/889]  Loss: 4.0567  Acc: 0.5266\n",
    "Epoch 1 [265/889]  Loss: 4.0542  Acc: 0.5270\n",
    "Epoch 1 [270/889]  Loss: 4.0509  Acc: 0.5297\n",
    "Epoch 1 [275/889]  Loss: 4.0480  Acc: 0.5307\n",
    "Epoch 1 [280/889]  Loss: 4.0449  Acc: 0.5338\n",
    "Epoch 1 [285/889]  Loss: 4.0418  Acc: 0.5359\n",
    "Epoch 1 [290/889]  Loss: 4.0385  Acc: 0.5387\n",
    "Epoch 1 [295/889]  Loss: 4.0354  Acc: 0.5405\n",
    "Epoch 1 [300/889]  Loss: 4.0329  Acc: 0.5418\n",
    "Epoch 1 [305/889]  Loss: 4.0296  Acc: 0.5440\n",
    "Epoch 1 [310/889]  Loss: 4.0261  Acc: 0.5464\n",
    "Epoch 1 [315/889]  Loss: 4.0232  Acc: 0.5484\n",
    "Epoch 1 [320/889]  Loss: 4.0199  Acc: 0.5504\n",
    "Epoch 1 [325/889]  Loss: 4.0175  Acc: 0.5516\n",
    "Epoch 1 [330/889]  Loss: 4.0149  Acc: 0.5532\n",
    "Epoch 1 [335/889]  Loss: 4.0119  Acc: 0.5552\n",
    "Epoch 1 [340/889]  Loss: 4.0089  Acc: 0.5567\n",
    "Epoch 1 [345/889]  Loss: 4.0059  Acc: 0.5587\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3971bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
