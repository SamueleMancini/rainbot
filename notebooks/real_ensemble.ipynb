{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAINBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "class ModelEnsembler(nn.Module):\n",
    "    def __init__(self, baseline_model, auxiliary_models: dict, feature_extractor, seg_model,\n",
    "                 segments_dict, num_classes, countries, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.baseline_model = baseline_model.eval().to(device)\n",
    "        self.auxiliary_models = {k: m.eval().to(device) for k, m in auxiliary_models.items()}\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.seg_model = seg_model.eval().to(device)\n",
    "        self.countries = countries\n",
    "        self.segments_dict = segments_dict\n",
    "        self.device = device\n",
    "\n",
    "        self.model_keys = ['baseline'] + list(auxiliary_models.keys())\n",
    "        self.raw_weights = nn.Parameter(torch.ones(len(self.model_keys)))  # Learnable\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, image_tensor):\n",
    "        return self.run(image_tensor)\n",
    "\n",
    "    def get_baseline_prediction(self, image_tensor):\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        baseline_logits = self.baseline_model(image_tensor)\n",
    "        return F.softmax(baseline_logits, dim=1)\n",
    "\n",
    "    def segmentation(self, image_tensor):\n",
    "        image = transforms.ToPILImage()(image_tensor.cpu())\n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.seg_model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits  # (1, num_classes, H, W)\n",
    "        pred = torch.argmax(logits.squeeze(), dim=0).cpu().numpy()  # (H, W)\n",
    "\n",
    "        # Resize original image\n",
    "        h, w = pred.shape\n",
    "        img_np = np.array(image.resize((w, h)))\n",
    "        segments = {}\n",
    "\n",
    "        for label_id, class_name in self.segments_dict.items():\n",
    "            mask = (pred == label_id)\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            canvas = np.zeros_like(img_np)\n",
    "            canvas[mask] = img_np[mask]\n",
    "            masked_img = Image.fromarray(canvas)\n",
    "\n",
    "            preprocess = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            segments[class_name] = preprocess(masked_img).to(self.device)\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def get_auxiliary_predictions(self, image_tensor):\n",
    "        preds = {}\n",
    "        segments = self.segmentation(image_tensor)\n",
    "\n",
    "        for cls, model in self.auxiliary_models.items():\n",
    "            if cls in segments:\n",
    "                crop = segments[cls].unsqueeze(0)  # (1, 3, H, W)\n",
    "                pred = model(crop)\n",
    "                preds[cls] = F.softmax(pred, dim=1)\n",
    "            else:\n",
    "                preds[cls] = torch.zeros((1, self.num_classes), device=self.device)\n",
    "        return preds\n",
    "\n",
    "    def ensemble_predictions(self, baseline_pred, auxiliary_preds):\n",
    "        weights = F.softmax(self.raw_weights, dim=0)\n",
    "        ensembled = weights[0] * baseline_pred\n",
    "        for i, key in enumerate(self.auxiliary_models.keys(), start=1):\n",
    "            ensembled += weights[i] * auxiliary_preds[key]\n",
    "        return ensembled, weights\n",
    "\n",
    "    def get_top5_predictions(self, ensembled):\n",
    "        top5_probs, top5_indices = torch.topk(ensembled, 5, dim=1)\n",
    "        return [self.countries[i] for i in top5_indices.squeeze(0)]\n",
    "\n",
    "    def run(self, image_tensor):\n",
    "        with torch.no_grad():\n",
    "            baseline_pred = self.get_baseline_prediction(image_tensor)\n",
    "            auxiliary_preds = self.get_auxiliary_predictions(image_tensor)\n",
    "            ensembled, weights = self.ensemble_predictions(baseline_pred, auxiliary_preds)\n",
    "            top5_countries = self.get_top5_predictions(ensembled)\n",
    "\n",
    "            return {\n",
    "                'baseline': baseline_pred,\n",
    "                'auxiliary': auxiliary_preds,\n",
    "                'weights': {k: float(w) for k, w in zip(self.model_keys, weights)},\n",
    "                'final_probs': ensembled,\n",
    "                'top5': top5_countries\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "# Compute absolute path to the `src/` folder\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "SRC_PATH     = os.path.join(PROJECT_ROOT, \"src\")\n",
    "\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "\n",
    "from utils import get_dataloaders, load_model, evaluate_model, print_metrics, plot_confusion_matrix, show_sample_predictions, plot_random_image_with_label_and_prediction, gradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COUNTRIES = [\"Albania\",\"Andorra\",\"Argentina\",\"Australia\",\"Austria\",\"Bangladesh\",\"Belgium\",\"Bhutan\",\"Bolivia\",\"Botswana\",\"Brazil\",\"Bulgaria\",\"Cambodia\",\"Canada\",\"Chile\",\"Colombia\",\"Croatia\",\"Czechia\",\"Denmark\",\"Dominican Republic\",\"Ecuador\",\"Estonia\",\"Eswatini\",\"Finland\",\"France\",\"Germany\",\"Ghana\",\"Greece\",\"Greenland\",\"Guatemala\",\"Hungary\",\"Iceland\",\"Indonesia\",\"Ireland\",\"Israel\",\"Italy\",\"Japan\",\"Jordan\",\"Kenya\",\"Kyrgyzstan\",\"Latvia\",\"Lesotho\",\"Lithuania\",\"Luxembourg\",\"Malaysia\",\"Mexico\",\"Mongolia\",\"Montenegro\",\"Netherlands\",\"New Zealand\",\"Nigeria\",\"North Macedonia\",\"Norway\",\"Palestine\",\"Peru\",\"Philippines\",\"Poland\",\"Portugal\",\"Romania\",\"Russia\",\"Senegal\",\"Serbia\",\"Singapore\",\"Slovakia\",\"Slovenia\",\"South Africa\",\"South Korea\",\"Spain\",\"Sri Lanka\",\"Sweden\",\"Switzerland\",\"Taiwan\",\"Thailand\",\"Turkey\",\"Ukraine\",\"United Arab Emirates\",\"United Kingdom\",\"United States\",\"Uruguay\"]\n",
    "num_classes = len(COUNTRIES)\n",
    "project_root   = Path().resolve().parent\n",
    "model = load_model(model_path=project_root / \"models\" / \"resnet_finetuned\" / \"main.pth\", device=device, num_classes=num_classes)\n",
    "aux = {}\n",
    "for segment in ['road', 'terrain', 'vegetation']:\n",
    "    aux['segment'] = model = load_model(model_path=project_root / \"models\" / f\"resnet_finetuned_{segment}\" / \"main.pth\", device=device, num_classes=num_classes)\n",
    "\n",
    "MODEL_NAME = \"nvidia/segformer-b0-finetuned-cityscapes-768-768\"\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "seg_model = SegformerForSemanticSegmentation.from_pretrained(MODEL_NAME).eval()\n",
    "\n",
    "segments_dict = {0: 'road', 8: 'vegetation',  9: 'terrain'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainbot = ModelEnsembler(model, aux, feature_extractor, seg_model, segments_dict, num_classes, COUNTRIES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/andreafabbricatore/rainbot/datasets/final_datasets/train/Andorra/cropped_0_AD_02015.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 30\u001b[0m country, logits \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_single_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted country: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m, in \u001b[0;36mprocess_single_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_single_image\u001b[39m(image_path):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Create transforms\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     11\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m     12\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     13\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], \n\u001b[1;32m     14\u001b[0m                            std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     15\u001b[0m     ])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Load and preprocess a single image\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def process_single_image(image_path):\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Create transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Apply transforms\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = rainbot(input_tensor)\n",
    "        predicted_class = torch.argmax(prediction, dim=1).item()\n",
    "        predicted_country = COUNTRIES[predicted_class]\n",
    "        \n",
    "    return predicted_country, prediction[0]\n",
    "\n",
    "# Example usage:\n",
    "image_path = \"/home/andreafabbricatore/rainbot/datasets/final_datasets/train/Andorra/cropped_0_AD_02015.jpg\"\n",
    "country, logits = process_single_image(image_path)\n",
    "print(f\"Predicted country: {country}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
