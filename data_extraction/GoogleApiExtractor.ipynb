{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "from PIL import Image\n",
    "from shapely.geometry import LineString\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries from which we will gather additional data from google static streetview api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES = ['Dominican Republic',\n",
    " 'Greenland',\n",
    " 'Luxembourg',\n",
    " 'Montenegro',\n",
    " 'North Macedonia',\n",
    " 'Albania',\n",
    " 'Palestine',\n",
    " 'Serbia',\n",
    " 'Lesotho',\n",
    " 'Kyrgyzstan',\n",
    " 'Guatemala',\n",
    " 'Jordan',\n",
    " 'Ecuador',\n",
    " 'Uruguay',\n",
    " 'Sri Lanka',\n",
    " 'Senegal',\n",
    " 'Mongolia']\n",
    "\n",
    "COUNTRY_ISO_MAP = {\n",
    "    'Dominican Republic': 'DO',\n",
    "    'Greenland': 'GL',\n",
    "    'Luxembourg': 'LU',\n",
    "    'Montenegro': 'ME',\n",
    "    'North Macedonia': 'MK',\n",
    "    'Albania': 'AL',\n",
    "    'Palestine': 'PS',\n",
    "    'Serbia': 'RS',\n",
    "    'Lesotho': 'LS',\n",
    "    'Kyrgyzstan': 'KG',\n",
    "    'Guatemala': 'GT',\n",
    "    'Jordan': 'JO',\n",
    "    'Ecuador': 'EC',\n",
    "    'Uruguay': 'UY',\n",
    "    'Sri Lanka': 'LK',\n",
    "    'Senegal': 'SN',\n",
    "    'Mongolia': 'MN'\n",
    "}\n",
    "\n",
    "country_name_fix = {\n",
    "\t\t'Dominican Republic': 'Dominican Rep.',\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving 10 biggest cities for each country. We will try to gather images close to big cities so that we will have more chances of getting coordinates with streetview photos available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top 10 cities per country to top_10_cities_per_country.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"geonames/worldcities.csv\")\n",
    "\n",
    "# Group by country and get top 10 most populous cities per country\n",
    "top_cities = (\n",
    "    df[df['population'].notnull()]  # exclude missing population\n",
    "    .sort_values(['country', 'population'], ascending=[True, False])\n",
    "    .groupby('country')\n",
    "    .head(10)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "top_cities = top_cities.rename(columns={\n",
    "    'country': 'country_full',\n",
    "    'iso2': 'country'\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "top_cities[['country_full', 'city', 'lat', 'lng', 'population', 'country']].to_csv(\"geonames/top_10_cities_per_country.csv\", index=False)\n",
    "\n",
    "print(\"Saved top 10 cities per country to top_10_cities_per_country.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\t\t\n",
    "API_KEY = \"\" \n",
    "MAX_TRIES = 500\n",
    "METADATA_RADIUS = 100  # meters\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# ---------------------------------------HELPER FUNCTIONS---------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- Download Natural Earth country borders ---\n",
    "def download_natural_earth(data_dir=\"./data\"):\n",
    "    url = \"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\"\n",
    "    zip_path = os.path.join(data_dir, \"countries.zip\")\n",
    "    shp_path = os.path.join(data_dir, \"ne_110m_admin_0_countries.shp\")\n",
    "\n",
    "    if os.path.exists(shp_path):\n",
    "        return shp_path\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    print(\"Downloading shapefile...\")\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        raise Exception(f\"Failed to download data. Status code: {r.status_code}\")\n",
    "\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "    except zipfile.BadZipFile:\n",
    "        os.remove(zip_path)\n",
    "        raise Exception(\"Downloaded file is not a valid ZIP archive.\")\n",
    "\n",
    "    os.remove(zip_path)\n",
    "    return shp_path\n",
    "\n",
    "\n",
    "def download_and_extract_cities500(data_dir=\"./geonames\"):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    zip_url = \"http://download.geonames.org/export/dump/cities500.zip\"\n",
    "    zip_path = os.path.join(data_dir, \"cities500.zip\")\n",
    "    extracted_path = os.path.join(data_dir, \"cities500.txt\")\n",
    "\n",
    "    if not os.path.exists(extracted_path):\n",
    "        print(\"Downloading cities500.zip...\")\n",
    "        r = requests.get(zip_url, stream=True)\n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(\"Downloaded. Extracting...\")\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "        os.remove(zip_path)\n",
    "        print(f\"Extracted to {extracted_path}\")\n",
    "    else:\n",
    "        print(\"cities500.txt already present.\")\n",
    "\n",
    "    return extracted_path\n",
    "\n",
    "\n",
    "def load_cities_for_country(country_code, cities_file=\"geonames/cities500.txt\"):\n",
    "    cols = [1, 2, 3, 5]\n",
    "    sep = ','\n",
    "    if cities_file == \"geonames/cities500.txt\":\n",
    "        cols = [1, 4, 5, 8]\n",
    "        sep = '\\t'\n",
    "    \n",
    "    df = pd.read_csv(cities_file, sep=sep, header=None, usecols=cols, names=[\"name\", \"lat\", \"lon\", \"country\"])\n",
    "    if country_code == \"all\":\n",
    "        return df\n",
    "    return df[df['country'] == country_code].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- Generate random point inside a country ---\n",
    "def get_random_point_near_city_in_country(country_name, gdf, cities_df, max_km=2):\n",
    "    \n",
    "    row = gdf[gdf['NAME'] == country_name]\n",
    "        \n",
    "    if country_name in country_name_fix:\n",
    "        normalized_name = country_name_fix.get(country_name, country_name)\n",
    "        row = gdf[gdf['NAME'] == normalized_name]\n",
    "    \n",
    "    if row.empty:\n",
    "        return None\n",
    "    \n",
    "    geom = row.iloc[0].geometry\n",
    "    iso_code = COUNTRY_ISO_MAP.get(country_name)\n",
    "    if not iso_code:\n",
    "        return None\n",
    "    \n",
    "    country_cities = cities_df[cities_df[\"country\"] == iso_code]\n",
    "    if country_cities.empty:\n",
    "        return None\n",
    "\n",
    "    for _ in range(100):  # max tries\n",
    "        city = country_cities.sample(1).iloc[0]\n",
    "        dx = random.normalvariate(0, 1) * max_km / 111\n",
    "        dy = random.normalvariate(0, 1) * max_km / 111\n",
    "        lon, lat = float(city[\"lon\"]) + dx, float(city[\"lat\"]) + dy\n",
    "        point = Point(lon, lat)\n",
    "        if geom.contains(point):\n",
    "            return point\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Query Street View metadata API ---\n",
    "def get_valid_streetview_metadata(lat, lon, api_key, radius=100, retries=5):\n",
    "    url = \"https://maps.googleapis.com/maps/api/streetview/metadata\"\n",
    "    params = {\n",
    "        \"location\": f\"{lat},{lon}\",\n",
    "        \"radius\": radius,\n",
    "        \"key\": api_key,\n",
    "        \"source\": \"outdoor\"\n",
    "    }\n",
    "\n",
    "    backoff = 0.2\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            if result.get(\"status\") == \"OK\":\n",
    "                return result\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Metadata API error on attempt {i+1}: {e}\")\n",
    "            time.sleep(backoff)\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_heading_from_road(lat, lon, distance=300, fallback_distance=600):\n",
    "    try:\n",
    "        G = ox.graph_from_point((lat, lon), dist=distance, network_type='drive')\n",
    "        edges = ox.graph_to_gdfs(G, nodes=False)\n",
    "        if edges.empty:\n",
    "            # Retry with larger distance\n",
    "            G = ox.graph_from_point((lat, lon), dist=fallback_distance, network_type='drive')\n",
    "            edges = ox.graph_to_gdfs(G, nodes=False)\n",
    "        if edges.empty:\n",
    "            raise ValueError(\"No roads found even in fallback distance\")\n",
    "\n",
    "        point = Point(lon, lat)\n",
    "        edges_proj = edges.to_crs(epsg=3857)\n",
    "        point_proj = gpd.GeoSeries([point], crs=\"EPSG:4326\").to_crs(epsg=3857).iloc[0]\n",
    "        nearest_edge = edges_proj.distance(point_proj).sort_values().index[0]\n",
    "        road = edges.loc[nearest_edge].geometry\n",
    "\n",
    "        if isinstance(road, LineString):\n",
    "            coords = list(road.coords)\n",
    "        elif road.geom_type == \"MultiLineString\":\n",
    "            coords = list(road.geoms[0].coords)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if len(coords) < 2:\n",
    "            return None\n",
    "        x1, y1 = coords[0]\n",
    "        x2, y2 = coords[-1]\n",
    "        \n",
    "        dx = x2 - x1\n",
    "        dy = y2 - y1\n",
    "        angle_rad = math.atan2(dx, dy)\n",
    "        heading = (math.degrees(angle_rad) + 360) % 360\n",
    "        return heading\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Heading computation error:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Download actual Street View image by pano_id ---\n",
    "def download_streetview_by_pano(pano_id, api_key, filename=\"streetview.jpg\", heading=random.randint(0, 360), fov=120, pitch=0):\n",
    "    url = \"https://maps.googleapis.com/maps/api/streetview\"\n",
    "    params = {\n",
    "        \"size\": \"640x640\",\n",
    "        \"pano\": pano_id,\n",
    "        \"heading\": heading,\n",
    "        \"fov\": fov,\n",
    "        \"pitch\": pitch,\n",
    "        \"key\": api_key,\n",
    "        \"source\": \"outdoor\"\n",
    "    }\n",
    "    r = requests.get(url, params=params)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return filename\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Detect if downloaded image is a blank placeholder ---\n",
    "def is_placeholder_image(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"L\")  # grayscale\n",
    "        arr = np.array(img)\n",
    "        std = arr.std()\n",
    "        return std < 2  # low variation = likely placeholder\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "\n",
    "# --- Find a valid Street View point inside the country ---\n",
    "def find_valid_streetview_point(country_name, gdf, cities_df,api_key, max_tries=1000):\n",
    "    for i in range(max_tries):\n",
    "        point = get_random_point_near_city_in_country(country_name, gdf, cities_df)\n",
    "        if point is None:\n",
    "            continue\n",
    "        lat, lon = point.y, point.x\n",
    "        metadata = get_valid_streetview_metadata(lat, lon, api_key, radius=METADATA_RADIUS)\n",
    "        if metadata:\n",
    "            pano_id = metadata.get(\"pano_id\")\n",
    "            if pano_id:\n",
    "                print(f\"Valid image after {i+1} tries: {country_name}\")\n",
    "                return point\n",
    "        time.sleep(0.1)  # rate limit safety\n",
    "    print(\"No valid image found.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Plot the result ---\n",
    "def plot_map_with_point(country_name, gdf, point):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    gdf.boundary.plot(ax=ax, color='gray')\n",
    "    \n",
    "    if country_name in country_name_fix:\n",
    "        country_name = country_name_fix.get(country_name, country_name)\n",
    "\n",
    "    gdf[gdf['NAME'] == country_name].plot(ax=ax, color='lightblue', edgecolor='blue')\n",
    "    if point:\n",
    "        ax.plot(point.x, point.y, 'ro', markersize=8)\n",
    "        ax.text(point.x, point.y, f\"({point.y:.4f}, {point.x:.4f})\", fontsize=8)\n",
    "    ax.set_title(f\"Street View Point in {country_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now extract all the images we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_IMAGE_COUNT = 250\n",
    "IMAGE_DIR = \"additional_images\"\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Loop across countries and collect images ---\n",
    "shp_path = download_natural_earth()\n",
    "cities_txt_path = download_and_extract_cities500()\n",
    "\n",
    "cities_df = load_cities_for_country(\"all\", \"geonames/top_10_cities_per_country.csv\")\n",
    "world = gpd.read_file(shp_path)\n",
    "\n",
    "MAX_IMAGES_PER_COUNTRY = 250\n",
    "country_image_count = defaultdict(int)\n",
    "for n in COUNTRIES:\n",
    "    country_image_count[n] = 0\n",
    "collected = 0\n",
    "\n",
    "print(f\"Collecting {TARGET_IMAGE_COUNT} total images...\")\n",
    "\n",
    "# Shuffle countries to start randomly\n",
    "country_pool = COUNTRIES.copy()\n",
    "random.shuffle(country_pool)\n",
    "limit = TARGET_IMAGE_COUNT * len(COUNTRIES) + 1\n",
    "\n",
    "\n",
    "while collected < limit:\n",
    "    for country in country_pool:\n",
    "        if country_image_count[country] >= MAX_IMAGES_PER_COUNTRY:\n",
    "            continue\n",
    "\n",
    "        print(f\"Searching in: {country}\")\n",
    "\n",
    "        point = find_valid_streetview_point(country, world, cities_df, API_KEY, max_tries=200)\n",
    "\n",
    "        if point:\n",
    "            lat, lon = point.y, point.x\n",
    "            metadata = get_valid_streetview_metadata(lat, lon, API_KEY, radius=METADATA_RADIUS)\n",
    "            pano_id = metadata.get(\"pano_id\")\n",
    "            \n",
    "            heading = compute_heading_from_road(lat, lon)\n",
    "            if heading is None:\n",
    "                  heading = 90\n",
    "\n",
    "            # Folder + filename\n",
    "            country_folder = os.path.join(IMAGE_DIR, country.replace(\" \", \"_\"))\n",
    "            os.makedirs(country_folder, exist_ok=True)\n",
    "            filename = os.path.join(country_folder, f\"{country.replace(' ', '_')}_{country_image_count[country]:03d}.jpg\")\n",
    "\n",
    "            saved = download_streetview_by_pano(pano_id, API_KEY, filename=filename, heading=heading)\n",
    "            if saved and not is_placeholder_image(filename):\n",
    "                print(f\"Saved #{collected+1}: {filename}\")\n",
    "                collected += 1\n",
    "                country_image_count[country] += 1\n",
    "            else:\n",
    "                print(\"Placeholder detected â€” skipping\")\n",
    "        else:\n",
    "            print(\"No valid location found in this round\")\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    # Reshuffle for next full round to ensure variety\n",
    "    random.shuffle(country_pool)\n",
    "\n",
    "print(\"Done! All images and metadata saved.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cubo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
